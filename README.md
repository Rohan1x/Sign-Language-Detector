# Sign Language Action Recognition ğŸ¤Ÿ

A real-time sign and gesture recognition system using **Mediapipe keypoints** and an **LSTM neural network** trained in TensorFlow.

---

## ğŸ§  Overview
This notebook trains and tests an action recognition model that can classify sign language gestures using sequential keypoint data.  
Itâ€™s designed for both recorded datasets and live webcam input.

## ğŸ“ Repository Structure
Sign-Language-Detection/
â”œâ”€ ActionRecognition.ipynb # Main notebook (training + testing)
â”œâ”€ requirements.txt # Dependencies list
â””â”€ Logs/
â””â”€ train/ # Training logs and metrics


## âš™ï¸ Setup & Usage
1. **Clone this repo**
   ```bash
   git clone https://github.com/Rohan1x/Sign-Language-Detection.git
   cd Sign-Language-Detection

Create and activate a virtual environment

python -m venv .venv
# Windows:
.venv\Scripts\activate
# macOS/Linux:
source .venv/bin/activate


Install dependencies

pip install -r requirements.txt


Launch Jupyter

jupyter lab   # or jupyter notebook


Then open ActionRecognition.ipynb and run each cell sequentially.
