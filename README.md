# Sign Language Action Recognition ğŸ¤Ÿ

A real-time sign and gesture recognition system using **Mediapipe keypoints** and an **LSTM neural network** trained in TensorFlow.

---

## ğŸ§  Overview
This notebook trains and tests an action recognition model that can classify sign language gestures using sequential keypoint data.  
Itâ€™s designed for both recorded datasets and live webcam input.

## ğŸ“ Repository Structure
Sign-Language-Detection/
â”œâ”€ ActionRecognition.ipynb # Main notebook (training + testing)
â”œâ”€ requirements.txt # Dependencies list
â””â”€ Logs/
â””â”€ train/ # Training logs and metrics


## âš™ï¸ Setup & Usage
1. **Clone this repo**
   ```bash
   git clone https://github.com/Rohan1x/Sign-Language-Detection.git
   cd Sign-Language-Detection

Create and activate a virtual environment

python -m venv .venv
# Windows:
.venv\Scripts\activate
# macOS/Linux:
source .venv/bin/activate


Install dependencies

pip install -r requirements.txt


Launch Jupyter

jupyter lab   # or jupyter notebook


Then open ActionRecognition.ipynb and run each cell sequentially.



ğŸ§© Features

Extracts pose/hand/face keypoints with Mediapipe Holistic

Preprocesses keypoint sequences for model input

Trains an LSTM model in TensorFlow/Keras

Evaluates accuracy and confusion matrix

Optional real-time webcam demo via OpenCV

ğŸ“Š Results

Validation accuracy â‰ˆ 93%

Low latency (<0.2 s per frame)

Logs automatically saved under Logs/train/

ğŸ§° Tools Used

Python Â· TensorFlow Â· Keras Â· Mediapipe Â· OpenCV Â· NumPy Â· Pandas Â· Matplotlib

ğŸ‘¤ Author

Rohan Domenguez
